---
title: "test"
author: "Ivan"
date: "2023-06-13"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, message=F}
library(processpredictR)
library(tidyverse)
library(bupaverse)
```

## Benchmark setup
### Datasets (event logs)

* As in gunnarssonDirectDataAware2023 (section 5. Experimental evaluation):
  + __BPIC12__
  + __BPIC17__
  + BPIC12-sub (events with the same activity labels as the previously occurring event are removed)
  + (BPIC19, BPIC20, BAG)


* Our setup:
  + __BPIC12__
  + __BPIC17__
  + BPIC12-sub (events with the same activity labels as the previously occurring event are removed)
  + traffic fines


### Model parameters (activities, traces, time durations, etc.):

* As in gunnarssonDirectDataAware2023 (section 5. Experimental evaluation):
  + abc
  + def
  
* Our setup:
  + ghi
  + jkl

### Some descriptive statistics on event logs and traces/variants are provided in table 3:

* name event log, sector
* #cases, #variants, avg. trace length (TL)
* Complexity event log: coverage (n=3), affinity/similarity (all traces) using normalized levenshtein diff. - 1
* _(structure, case features, event features, variability)_

> "Observe that __time independent__ data splits were applied when training and evaluating the considered models. Hereto, cases in the event logs were first __order based on their timestamps__. For a majority of the
event logs, the last occurring quarter of cases was chosen as a test set and the remaining cases were used to train the model. __Cases in the training set that were still ongoing when the first case in the test set was being executed were removed, resulting in a time independent training and test set__. Lastly, a quarter of the time independent training set was randomly sampled and used as a validation set." - gunnarssonDirectDataAware2023



## Prep
Load the datasets
```{r}
# xesreadR::read_xes("data/BPI_Challenge_2012.xes.gz") -> raw_df_bpi2012
# xesreadR::read_xes("data/BPI Challenge 2017.xes.gz") -> raw_df_bpi2017 # does not read (memory related issue)
eventdataR::traffic_fines -> raw_df_traffic_fines
readRDS(file = "/Users/ninaesina/Desktop/BIARU/ICPM23_RemainingTraceAndOutcomePredictionWithTransformers/data/saved/raw_df_bpi2012.Rds") -> raw_df_bpi2012
# readRDS(file = "data/saved/raw_df_traffic_fines.Rds") -> raw_df_traffic_fines
# saveRDS(raw_df_bpi2012, "data/saved/raw_df_bpi2012.Rds")
# saveRDS(raw_df_traffic_fines, "data/saved/raw_df_traffic_fines.Rds")
```

Check for cases with trace length of 1 (and remove)
```{r}
raw_df_traffic_fines %>% group_by(case_id) %>% summarise(n =n()) %>% filter(n <= 1)
```

### Traffic fines dataset
Transform dataset into 'prefix' dataset using `prepare_examples()`. The first 6 rows are shown below.
```{r}
processpredictR::prepare_examples(raw_df_traffic_fines, task = "remaining_trace_s2s") -> ex_traffic_fines
```

```{r, echo=FALSE}
ex_traffic_fines %>% head
```

#### Train-, test- and validation split
Split dataset into train-, validation- (last occurring quarter of the train set) and test set (last occurring quarter of the full sample). How does `train_test_split()` works in `processpredictR`:
```{r}
# Split the full 'prefix' dataset into train (0.75) and test (0.25) datasets
ex_traffic_fines %>% split_train_test(split = 0.75) -> train_test

# Split the train dataset into train (0.75) and validation (0.25) datasets
train_test$train_df %>% split_train_test(split = 0.75) -> train_validation

# Reassign for ease of readability
TF_train <- train_validation$train_df
TF_valid <- train_validation$test_df
TF_test <- train_test$test_df
```

__Limitation/complexity of the split: the unique traces present in the test (and validation) set may not necessarily have appeared in the train set. In other words, it is possible that the trained model has never seen the behaviour, in particular traces, that is present in the later observations from the test set. (Optionally add shuffling of cases argument, when splitting?)__


Let's check if there are such traces. 
```{r, echo=F, message=F}
cat("Number of unique prefixes:", "\n", 
             "Train set - ", TF_train$prefix %>% n_distinct(), "\n",
             "Test set - ", TF_test$prefix %>% n_distinct(),
    sep = ""
      )

list(TF_train=TF_train, TF_test=TF_test) %>% 
  map(~
  group_by(.x, case_id) %>% 
  filter(k == max(k)) %>% # keep only the fullest prefix for each case (= trace)
  ungroup() %>%
  distinct(prefix)) -> list_unique_traces_TF_train_test

cat("\nNumber of unique traces:",
    "\nTrain set - ", list_unique_traces_TF_train_test$TF_train %>% nrow,
    "\nTest set - ", list_unique_traces_TF_train_test$TF_test %>% nrow,
    "\nBoth sets - ", list_unique_traces_TF_train_test$TF_train %>%
      mutate(set = "train") %>%
      full_join(list_unique_traces_TF_train_test$TF_test) %>% nrow,
    sep = "")

list_unique_traces_TF_train_test$TF_train %>%
  mutate(set = "train") %>%
  full_join(list_unique_traces_TF_train_test$TF_test %>% 
               mutate(set = "test"), by = "prefix") %>% 
  mutate(set.x = if_else(is.na(set.x), "", set.x), 
         set.y = if_else(is.na(set.y), "", set.y)) %>%
  unite(col = set, c("set.x", "set.y"), sep = "") %>% 
  group_by(set) %>% 
  summarise(n = n()) -> tmp

cat("\nNumber of exclusively unique traces:",
    "\nTrain set - ", tmp %>% filter(set == "train") %>% pull(n),
    "\nTest set - ", tmp %>% filter(set == "test") %>% pull(n),
    "\nBoth sets - ", tmp %>% filter(set == "traintest") %>% pull(n))
```

> _Thus, there are 7 traces that are only exclusive to the test set. In other words, they are not present in the train set and the model has not been trained on them. One would question how it affects the performance of the model (measured in Levenshtein difference)? In that case, one would assume that the overall accuracy of the predicted traces would depend on how much (Lichtenshtein diff.) the traces in the train set are different from those in the test set. For example, a trace (a, b, c) that only occurs in the test set is relatively close to the traces (a, b), (a, c) and (b, c) that are not necessarily exclusive to the train set. In that case, although the particular trace from the test set has not been observed in the train set, it still would be able to predict reasonably well?_

#### Remaining trace transformer model
Setting the hyperparameters of the model, 5 different configurations: 

* base (8h) w/ 1 layer
* base (4h) w/ 1 layer
* Small w/ 2 layer
* Medium w/ 2 layer
* Large w/ 2 layer

__Added num_layers__ hyperparameter to `processpredictR`-package.

* Base (8h) config with 8 attention heads:
  + num_layers = 1
  + num_heads = 8
  + d_model = 36
  + dff = 64
  + dropout_rate = 0.1
  
```{r, eval=T, echo=T, message=F}
ex_traffic_fines %>% create_model(num_heads = 8, output_dim = 36, dim_ff = 64, num_layers = 1) -> model_0_8h_traffic_fines
model_0_8h_traffic_fines
```
  
* Base (4h) config with 4 attention heads:
  + num_layers = 1
  + __num_heads = 4__
  + d_model = 36
  + dff = 64
  + dropout_rate = 0.1
  
```{r, eval=T, echo=T, message=F}
ex_traffic_fines %>% create_model(num_heads = 4, output_dim = 36, dim_ff = 64, num_layers = 1) -> model_0_4h_traffic_fines
model_0_4h_traffic_fines
```
  
* Small config with 2 stacked layers (encoder-decoder):
  + __num_layers = 2__
  + num_heads = 8
  + d_model = 36
  + dff = 64
  + dropout_rate = 0.1
  
```{r, eval=T, echo=T, message=F}
ex_traffic_fines %>% create_model(num_heads = 8, output_dim = 36, dim_ff = 64, num_layers = 2) -> model_0_8h_2l_traffic_fines
model_0_8h_2l_traffic_fines
```


* Medium config (M):
  + num_layers = 2
  + num_heads = 8
  + __d_model = 128__
  + __dff = 512__
  + dropout_rate = 0.1
  
```{r, eval=T, echo=T, message=F}
ex_traffic_fines %>% create_model(num_heads = 8, output_dim = 128, dim_ff = 512, num_layers = 2) -> model_0_8h_2l_M_traffic_fines
model_0_8h_2l_M_traffic_fines
```
  
* Large config (L):
  + num_layers = 2
  + num_heads = 8
  + __d_model = 512__
  + __dff = 2048__
  + dropout_rate = 0.1

```{r, eval=T, echo=T, message=F}
ex_traffic_fines %>% create_model(num_heads = 8, output_dim = 512, dim_ff = 2048, num_layers = 1) -> model_0_8h_2l_L_traffic_fines
model_0_8h_2l_L_traffic_fines
```



Compile models.
```{r, eval=T}
# 1 layer (Base-Small) with varying number of attention heads
compile(model_0_8h_traffic_fines)
compile(model_0_4h_traffic_fines)

# 2 layers
compile(model_0_8h_2l_traffic_fines) # Small
compile(model_0_8h_2l_M_traffic_fines) # Medium
compile(model_0_8h_2l_L_traffic_fines) # Large
```

Train models.
Model TF0_8h.
```{r, eval=F}
histTF_0_8h <- fit(object = model_0_8h_traffic_fines, train_data = TF_train, epochs = 50,
             callbacks = list(early_stopping))
```

Model TF0_4h.
```{r, eval=F}
histTF_0_4h <- fit(object = model_0_4h_traffic_fines, train_data = TF_train, epochs = 50,
             callbacks = list(early_stopping))
```


Model TF0_8h_2l.
```{r, eval=F}
histTF_0_8h_2l <- fit(object = model_0_8h_2l_traffic_fines, train_data = TF_train, epochs = 50,
             callbacks = list(early_stopping))
```

poch 1/50
1572/1572 [==============================] - 51s 29ms/step - loss: 0.1701 - sparse_categorical_accuracy: 0.9383 - val_loss: 0.1097 - val_sparse_categorical_accuracy: 0.9597
Epoch 2/50
1572/1572 [==============================] - 58s 37ms/step - loss: 0.1178 - sparse_categorical_accuracy: 0.9532 - val_loss: 0.1085 - val_sparse_categorical_accuracy: 0.9597
Epoch 3/50
1572/1572 [==============================] - 60s 38ms/step - loss: 0.1136 - sparse_categorical_accuracy: 0.9544 - val_loss: 0.1012 - val_sparse_categorical_accuracy: 0.9596
Epoch 4/50
1572/1572 [==============================] - 61s 39ms/step - loss: 0.1122 - sparse_categorical_accuracy: 0.9549 - val_loss: 0.1025 - val_sparse_categorical_accuracy: 0.9601
Epoch 5/50
1572/1572 [==============================] - 60s 38ms/step - loss: 0.1109 - sparse_categorical_accuracy: 0.9551 - val_loss: 0.1024 - val_sparse_categorical_accuracy: 0.9601
Epoch 6/50
1572/1572 [==============================] - 59s 38ms/step - loss: 0.1098 - sparse_categorical_accuracy: 0.9553 - val_loss: 0.1009 - val_sparse_categorical_accuracy: 0.9598
Epoch 7/50
1572/1572 [==============================] - 59s 37ms/step - loss: 0.1097 - sparse_categorical_accuracy: 0.9557 - val_loss: 0.1037 - val_sparse_categorical_accuracy: 0.9599
Epoch 8/50
1572/1572 [==============================] - 57s 36ms/step - loss: 0.1121 - sparse_categorical_accuracy: 0.9551 - val_loss: 0.1020 - val_sparse_categorical_accuracy: 0.9601
Epoch 9/50
1572/1572 [==============================] - 58s 37ms/step - loss: 0.1079 - sparse_categorical_accuracy: 0.9559 - val_loss: 0.1009 - val_sparse_categorical_accuracy: 0.9600
Epoch 10/50
1572/1572 [==============================] - 56s 36ms/step - loss: 0.1080 - sparse_categorical_accuracy: 0.9560 - val_loss: 0.1003 - val_sparse_categorical_accuracy: 0.9599
Epoch 11/50
1572/1572 [==============================] - 55s 35ms/step - loss: 0.1088 - sparse_categorical_accuracy: 0.9554 - val_loss: 0.1005 - val_sparse_categorical_accuracy: 0.9601
Epoch 12/50
1572/1572 [==============================] - 54s 35ms/step - loss: 0.1080 - sparse_categorical_accuracy: 0.9559 - val_loss: 0.1022 - val_sparse_categorical_accuracy: 0.9602
Epoch 13/50
1572/1572 [==============================] - 57s 36ms/step - loss: 0.1077 - sparse_categorical_accuracy: 0.9560 - val_loss: 0.1021 - val_sparse_categorical_accuracy: 0.9505
Epoch 14/50
1572/1572 [==============================] - 56s 35ms/step - loss: 0.1078 - sparse_categorical_accuracy: 0.9558 - val_loss: 0.1037 - val_sparse_categorical_accuracy: 0.9602
Epoch 15/50
1572/1572 [==============================] - 56s 35ms/step - loss: 0.1071 - sparse_categorical_accuracy: 0.9562 - val_loss: 0.1004 - val_sparse_categorical_accuracy: 0.9602
Epoch 16/50
1572/1572 [==============================] - 55s 35ms/step - loss: 0.1068 - sparse_categorical_accuracy: 0.9562 - val_loss: 0.0997 - val_sparse_categorical_accuracy: 0.9601
Epoch 17/50
1572/1572 [==============================] - 55s 35ms/step - loss: 0.1082 - sparse_categorical_accuracy: 0.9556 - val_loss: 0.1010 - val_sparse_categorical_accuracy: 0.9596
Epoch 18/50
1572/1572 [==============================] - 55s 35ms/step - loss: 0.1060 - sparse_categorical_accuracy: 0.9563 - val_loss: 0.0999 - val_sparse_categorical_accuracy: 0.9604
Epoch 19/50
1572/1572 [==============================] - 56s 36ms/step - loss: 0.1061 - sparse_categorical_accuracy: 0.9563 - val_loss: 0.1019 - val_sparse_categorical_accuracy: 0.9601
Epoch 20/50
1572/1572 [==============================] - 56s 35ms/step - loss: 0.1077 - sparse_categorical_accuracy: 0.9560 - val_loss: 0.1004 - val_sparse_categorical_accuracy: 0.9599
Epoch 21/50
1572/1572 [==============================] - 56s 35ms/step - loss: 0.1063 - sparse_categorical_accuracy: 0.9560 - val_loss: 0.0998 - val_sparse_categorical_accuracy: 0.9597
Epoch 22/50
1572/1572 [==============================] - 42s 27ms/step - loss: 0.1072 - sparse_categorical_accuracy: 0.9559 - val_loss: 0.0995 - val_sparse_categorical_accuracy: 0.9601
Epoch 23/50
1572/1572 [==============================] - 36s 23ms/step - loss: 0.1059 - sparse_categorical_accuracy: 0.9562 - val_loss: 0.0998 - val_sparse_categorical_accuracy: 0.9600
Epoch 24/50
1572/1572 [==============================] - 37s 24ms/step - loss: 0.1059 - sparse_categorical_accuracy: 0.9564 - val_loss: 0.0984 - val_sparse_categorical_accuracy: 0.9602
Epoch 25/50
1572/1572 [==============================] - 38s 24ms/step - loss: 0.1062 - sparse_categorical_accuracy: 0.9560 - val_loss: 0.1012 - val_sparse_categorical_accuracy: 0.9600
Epoch 26/50
1572/1572 [==============================] - 37s 24ms/step - loss: 0.1062 - sparse_categorical_accuracy: 0.9563 - val_loss: 0.1018 - val_sparse_categorical_accuracy: 0.9601
Epoch 27/50
1572/1572 [==============================] - 38s 24ms/step - loss: 0.1078 - sparse_categorical_accuracy: 0.9556 - val_loss: 0.1006 - val_sparse_categorical_accuracy: 0.9602
Epoch 28/50
1572/1572 [==============================] - 37s 24ms/step - loss: 0.1059 - sparse_categorical_accuracy: 0.9563 - val_loss: 0.1009 - val_sparse_categorical_accuracy: 0.9596
Epoch 29/50
1572/1572 [==============================] - 38s 24ms/step - loss: 0.1058 - sparse_categorical_accuracy: 0.9564 - val_loss: 0.0994 - val_sparse_categorical_accuracy: 0.9601
Epoch 30/50
1572/1572 [==============================] - 38s 24ms/step - loss: 0.1063 - sparse_categorical_accuracy: 0.9562 - val_loss: 0.1005 - val_sparse_categorical_accuracy: 0.9602
Epoch 31/50
1572/1572 [==============================] - 34s 22ms/step - loss: 0.1063 - sparse_categorical_accuracy: 0.9561 - val_loss: 0.0990 - val_sparse_categorical_accuracy: 0.9601
Epoch 32/50
1572/1572 [==============================] - 33s 21ms/step - loss: 0.1055 - sparse_categorical_accuracy: 0.9563 - val_loss: 0.0999 - val_sparse_categorical_accuracy: 0.9598
Epoch 33/50
1572/1572 [==============================] - 36s 23ms/step - loss: 0.1054 - sparse_categorical_accuracy: 0.9564 - val_loss: 0.0999 - val_sparse_categorical_accuracy: 0.9602
Epoch 34/50
1572/1572 [==============================] - 33s 21ms/step - loss: 0.1058 - sparse_categorical_accuracy: 0.9562 - val_loss: 0.1007 - val_sparse_categorical_accuracy: 0.9601

Model TF0_8h_2l_M.
```{r, eval=F}
histTF_0_8h_2l_M <- fit(object = model_0_8h_2l_M_traffic_fines, train_data = TF_train, epochs = 50,
             callbacks = list(early_stopping))
```


Epoch 1/50
1572/1572 [==============================] - 186s 112ms/step - loss: 123.9451 - sparse_categorical_accuracy: 0.8822 - val_loss: 4.1140 - val_sparse_categorical_accuracy: 0.3635
Epoch 2/50
1572/1572 [==============================] - 154s 98ms/step - loss: 2.3238 - sparse_categorical_accuracy: 0.7725 - val_loss: 0.4297 - val_sparse_categorical_accuracy: 0.8395
Epoch 3/50
1572/1572 [==============================] - 150s 95ms/step - loss: 0.5058 - sparse_categorical_accuracy: 0.8482 - val_loss: 0.3021 - val_sparse_categorical_accuracy: 0.8919
Epoch 4/50
1572/1572 [==============================] - 134s 85ms/step - loss: 875.6709 - sparse_categorical_accuracy: 0.8135 - val_loss: 83.5519 - val_sparse_categorical_accuracy: 0.1046
Epoch 5/50
1572/1572 [==============================] - 121s 77ms/step - loss: 88.3903 - sparse_categorical_accuracy: 0.6212 - val_loss: 10.6958 - val_sparse_categorical_accuracy: 0.5279
Epoch 6/50
1572/1572 [==============================] - 121s 77ms/step - loss: 187.9077 - sparse_categorical_accuracy: 0.7064 - val_loss: 22.2623 - val_sparse_categorical_accuracy: 0.7393
Epoch 7/50
1572/1572 [==============================] - 125s 79ms/step - loss: 33.5006 - sparse_categorical_accuracy: 0.7801 - val_loss: 3.7076 - val_sparse_categorical_accuracy: 0.6661
Epoch 8/50
1572/1572 [==============================] - 124s 79ms/step - loss: 485.6763 - sparse_categorical_accuracy: 0.7321 - val_loss: 22.8732 - val_sparse_categorical_accuracy: 0.6675
Epoch 9/50
1572/1572 [==============================] - 125s 79ms/step - loss: 638.9536 - sparse_categorical_accuracy: 0.6806 - val_loss: 50.8946 - val_sparse_categorical_accuracy: 0.6842
Epoch 10/50
1572/1572 [==============================] - 126s 80ms/step - loss: 4566.7290 - sparse_categorical_accuracy: 0.6244 - val_loss: 87.6752 - val_sparse_categorical_accuracy: 0.3766
Epoch 11/50
1572/1572 [==============================] - 125s 79ms/step - loss: 155.1564 - sparse_categorical_accuracy: 0.6464 - val_loss: 43.6324 - val_sparse_categorical_accuracy: 0.4077
Epoch 12/50
1572/1572 [==============================] - 126s 80ms/step - loss: 3619.1345 - sparse_categorical_accuracy: 0.6177 - val_loss: 637.4479 - val_sparse_categorical_accuracy: 0.7393
Epoch 13/50
1572/1572 [==============================] - 124s 79ms/step - loss: 742.9449 - sparse_categorical_accuracy: 0.6058 - val_loss: 66.2127 - val_sparse_categorical_accuracy: 0.3530


Model TF0_8h_2l_L.
```{r, eval=F}
histTF_0_8h_2l_L <- fit(object = model_0_8h_2l_L_traffic_fines, train_data = TF_train, epochs = 50,
             callbacks = list(early_stopping))
```


```{r, echo=F, eval=F}
# model_0_8h_traffic_fines
# model_0_4h_traffic_fines
# model_0_8h_2l_traffic_fines$model %>% save_model_tf("models")
```


Predictions.
```{r, eval=F}
predict(model_0_8h_traffic_fines, TF_test[1, ], output = "append")
predict(model_0_8h_traffic_fines, TF_test[1, ], output = "append")
predict(model_0_8h_traffic_fines, TF_test[1, ], output = "append")
```



```{r, eval=F, echo=F}
# # Custom learning rate schedule
# CustomSchedule <- R6Class("CustomSchedule",
#                           public = list(
#                             d_model = NULL,
#                             warmup_steps = NULL,
# 
#                             initialize = function(d_model, warmup_steps = 4000) {
#                               self$d_model <- as.numeric(d_model)
#                               self$warmup_steps <- warmup_steps
#                             },
# 
#                             call = function(step) {
#                               step <- as.numeric(step)
#                               arg1 <- sqrt(step)
#                               arg2 <- step * (self$warmup_steps ^ -1.5)
# 
#                               sqrt(self$d_model) * min(arg1, arg2)
#                             }
#                           )
# )

custom_schedule <- function(d_model, warmup_steps = 4000) {
  function(step) {
    step <- as.numeric(step)
    arg1 <- sqrt(step)
    arg2 <- step * (warmup_steps ^ -1.5)

    sqrt(d_model) * min(arg1, arg2)
  }
}


# Adam optimizer with custom learning rate
# learning_rate <- CustomSchedule$new(128) # d_model
learning_rate <- custom_schedule(128)  # d_model

optimizer <- keras::optimizer_adam(
  # learning_rate = learning_rate,
  beta_1 = 0.9,
  beta_2 = 0.98,
  epsilon = 1e-9
)

# Custom loss function
masked_loss <- function(label, pred) {
  mask <- label != 0
  loss_object <- keras::loss_sparse_categorical_crossentropy(
    from_logits = TRUE, 
    reduction = "none"
  )
  loss <- loss_object(label, pred)

  mask <- as.numeric(mask)
  loss <- loss * mask

  sum(loss) / sum(mask)
}

# Custom accuracy function
masked_accuracy <- function(label, pred) {
  pred <- argmax(pred, axis = 2)
  label <- as.numeric(label)
  match <- label == pred

  mask <- label != 0

  match <- match & mask

  sum(match) / sum(mask)
}


masked_loss <- function(label, pred) {
  mask <- keras::k_cast(label, "bool")
  loss_object <- keras::loss_sparse_categorical_crossentropy(
    from_logits = TRUE, 
    reduction = "none"
  )
  loss <- loss_object(label, pred)

  loss <- loss * keras::k_cast(mask, "float32")

  keras::k_sum(loss) / keras::k_sum(keras::k_cast(mask, "float32"))
}

# Custom accuracy function
masked_accuracy <- function(label, pred) {
  pred <- keras::k_argmax(pred, axis = 2)
  mask <- keras::k_cast(keras::k_not_equal(label, 0), "float32")

  match <- keras::k_cast(keras::k_equal(label, pred), "float32")
  match <- match * mask

  keras::k_sum(match) / keras::k_sum(mask)
}


model_0_traffic_fines$model %>% 
  keras::compile(
    loss = masked_loss,
    optimizer = optimizer,
    metrics = list(masked_accuracy)
  )
```

```{r, eval=F, echo=F}
# Let's see how custom masked loss and masked accuracy affect the results.
hist0_custom_loss_metric <- fit(object = model_0_traffic_fines, train_data = TF_train, epochs = 50,
             callbacks = list(early_stopping))

```


```{r, eval=F, echo=F}
TF_train %>% prep_remaining_trace2() -> tmptftrain
keras::fit(model_S_traffic_fines$model,
           list(tmptftrain$current_tokens, tmptftrain$remaining_tokens),
           tmptftrain$remaining_tokens_shifted,
           epochs = 20,
           callbacks = list(early_stopping))

histS <- fit(object = model_S_traffic_fines, train_data = TF_train, epochs = 50,
             callbacks = list(early_stopping))
```

---


### bpic12 dataset
Transform dataset into 'prefix' dataset using `prepare_examples()`. The first 6 rows are shown below.
```{r}
processpredictR::prepare_examples(raw_df_bpi2012, task = "remaining_trace_s2s") -> ex_bpi2012
```

#### Train-, test- and validation split
Split dataset into train-, validation- (last occurring quarter of the train set) and test set (last occurring quarter of the full sample). How does `train_test_split()` works in `processpredictR`:
```{r}
# Split the full 'prefix' dataset into train (0.75) and test (0.25) datasets
ex_bpi2012 %>% split_train_test(split = 0.75) -> train_test_bpi2012

# Split the train dataset into train (0.75) and validation (0.25) datasets
train_test_bpi2012$train_df %>% split_train_test(split = 0.75) -> train_validation_bpi2012

# Reassign for ease of readability
bpi2012_train <- train_validation_bpi2012$train_df
bpi2012_valid <- train_validation_bpi2012$test_df
bpi2012_test <- train_test_bpi2012$test_df
```

```{r, echo=F, message=F}
cat("Number of unique prefixes:", "\n", 
             "Train set - ", bpi2012_train$prefix %>% n_distinct(), "\n",
             "Test set - ", bpi2012_test$prefix %>% n_distinct(),
    sep = ""
      )

list(bpi2012_train=bpi2012_train, bpi2012_test=bpi2012_test) %>% 
  map(~
  group_by(.x, CASE_concept_name) %>% 
  filter(k == max(k)) %>% # keep only the fullest prefix for each case (= trace)
  ungroup() %>%
  distinct(prefix)) -> list_unique_traces_bpi2012_train_test

cat("\nNumber of unique traces:",
    "\nTrain set - ", list_unique_traces_bpi2012_train_test$bpi2012_train %>% nrow,
    "\nTest set - ", list_unique_traces_bpi2012_train_test$bpi2012_test %>% nrow,
    "\nBoth sets - ", list_unique_traces_bpi2012_train_test$bpi2012_train %>%
      mutate(set = "train") %>%
      full_join(list_unique_traces_bpi2012_train_test$bpi2012_test) %>% nrow,
    sep = "")

list_unique_traces_bpi2012_train_test$bpi2012_train %>%
  mutate(set = "train") %>%
  full_join(list_unique_traces_bpi2012_train_test$bpi2012_test %>% 
               mutate(set = "test"), by = "prefix") %>% 
  mutate(set.x = if_else(is.na(set.x), "", set.x), 
         set.y = if_else(is.na(set.y), "", set.y)) %>%
  unite(col = set, c("set.x", "set.y"), sep = "") %>% 
  group_by(set) %>% 
  summarise(n = n()) -> tmp

cat("\nNumber of exclusively unique traces:",
    "\nTrain set - ", tmp %>% filter(set == "train") %>% pull(n),
    "\nTest set - ", tmp %>% filter(set == "test") %>% pull(n),
    "\nBoth sets - ", tmp %>% filter(set == "traintest") %>% pull(n))
```


#### Remaining trace transformer model
* Base (0) config with 8 attention heads:
  + num_layers = 1
  + num_heads = 8
  + d_model = 36
  + dff = 64
  + dropout_rate = 0.1
  
```{r, eval=T, echo=T, message=F}
ex_bpi2012 %>% create_model(num_heads = 8, output_dim = 36, dim_ff = 64, num_layers = 1) -> model_0_8h_bpi2012
model_0_8h_bpi2012
```

---

Compile models.
```{r, eval=T}
# 1 layer (Base-Small) with varying number of attention heads
compile(model_0_8h_bpi2012)
# compile(model_0_4h_bpi2012)

# # 2 layers
# compile(model_0_8h_2l_bpi2012) # Small
# compile(model_0_8h_2l_M_bpi2012) # Medium
# compile(model_0_8h_2l_L_bpi2012) # Large
```

Train models.
Model bpi2012_0_8h.
```{r, eval=F}
hist_bpi2012_0_8h <- fit(object = model_0_8h_bpi2012, train_data = bpi2012_train, epochs = 50,
             callbacks = list(early_stopping))
```
Epoch 1/50
11850/11850 [==============================] - 4663s 393ms/step - loss: 0.0796 - sparse_categorical_accuracy: 0.9757 - val_loss: 0.0620 - val_sparse_categorical_accuracy: 0.9806
Epoch 2/50
 1494/11850 [==>...........................] - ETA: 1:02:19 - loss: 0.0620 - sparse_categorical_accuracy: 0.9805

















