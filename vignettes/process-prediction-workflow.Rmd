---
title: "Introduction to processpredictR: workflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{process-prediction-workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Prologue
The goal of processpredictR is to perform prediction tasks on processes using event logs and Transformer models.
5 process monitoring tasks are defined as follows:

* outcome: 
* next activity: 
* remaining trace: 
* next time: 
* remaining time: 

```{r setup}
library(processpredictR)
library(eventdataR)
library(ggplot2)
library(dplyr)
library(keras)
library(purrr)
```

# Preprocessing
As a first step in the process prediction workflow we must obtain a dataset of the following format, where: 

* each row/observation is a unique instance id
* _(an event log is implicitly transformed to activity log)_
* the prefix column is a cumulative sum/concatenation of activities in a trace (of a case)
```{r}
df <- prepare_examples(patients, task = "outcome")
df
```

We split the transformed dataset into train- and test sets for a later use in fit() and predict(), respectively. 
```{r}
set.seed(123)
split <- df %>% split_train_test(trace_length_bins = 5)
split$train_df %>% head(5)
split$test_df %>% head(5)
```
It's important to note that the split is done randomly^[whether is it representative to mix up the observed sequence of cases in an event log is uncertain at the moment of writing] and by cases (not by rows). The latter normally would imply that the row level split ratio should not necessarily correspond to the case level split ratio. That is because the trace lengths can differ. To control for that the trace_length_bins argument is provided and applied by default.
```{r}
nrow(split$train_df) / nrow(df)
n_distinct(split$train_df$patient) / n_distinct(df$patient)
```

# Transformer model
Next step in the workflow is to build/define a model. processpredictR provides a default set of functions that are wrappers of generics provided by keras-package. For the easy of use, the preprocessing steps such as tokenizing of sequences, normalizing numerical features, etc. are automated. 

## Define model
Based on the train set we define the default transformer model.
```{r}
model <- split$train_df %>% create_model(custom = FALSE, 
                                         name = "my_model") # pass arguments as ... that are applicable to keras::keras_model()

model # is a list 

model %>% attributes() # objects from a returned list
```

Additionally, we can use functions from the keras-package as follows:
```{r}
model$model$name # get the name of a model
model$model$non_trainable_variables # list of non-trainable parameters of a model
```

The model is assigned it's own class for which the processpredictR provides methods of _compile()_, _fit()_, _predict()_ and _evaluate()_. 
```{r, message=FALSE}
model %>% class
methods(compile)
```

## Compilation
We compile the model. By default, the loss function is the log-cosh and the categorical cross entropy for regression tasks (next time and remaining time) and classification tasks, respectively. It is of course possible to override the default arguments.
```{r}
model %>% compile() # model compilation
```

## Training
Optionally assign it to an object to access the training metrics specified in _compile()_. 
```{r, cache=TRUE}
hist <- fit(object = model, train_data = split$train_df, epochs = 5)
hist$params
hist$metrics
```

## Make predictions
The defined method for _predict()_ can return 3 types of output: 
Test dataset with appended predicted values (output = "append")
```{r}
predictions <- model %>% predict(test_data = split$test_df, 
                                 output = "append") # default
predictions %>% head(5)
```

<details>
<summary>raw predicted values (output = "raw")</summary>
<p>
```{r, echo=FALSE}
model %>% predict(test_data = split$test_df,
                  output = "raw")
```
</details>
</p>

<details>
<summary>predicted values with postprocessing (output = "y_pred")</summary>
```{r, echo=FALSE}
model %>% predict(test_data = split$test_df,
                  output = "y_pred")
```
</details>
</p>

### Visualize predictions
For the classification tasks outcome and next activity a confusion_matrix function is provided.
```{r}
predictions %>% class
confusion_matrix(predictions)
```

Plot method for the confusion matrix (classification) or a scatter plot (regression).
```{r}
plot(predictions) %>% coord_flip()
```

## Evaluate model
Returns loss and metrics specified in _compile()_.
```{r}
model %>% evaluate(split$test_df)
```

# Add extra features to your model
```{r}
# preprocessed dataset with categorical hot encoded features
df_next_time <- patients %>% prepare_examples(task = "next_time", features = "employee") %>% split_train_test()
df_next_time$train_df %>% head(5)

# the attributes of df are added or changed accordingly
df_next_time$train_df %>% attr("numeric_features")

# create a model
df_next_time_model <- create_model(df_next_time$train_df)
df_next_time_model

# access other useful elements from a returned list
df_next_time$train_df %>% attr("features")
df_next_time$train_df %>% attr("hot_encoded_categorical_features")
```

# Build your own transformer model
```{r}
df <- prepare_examples(patients, task = "next_activity") %>% split_train_test()
custom_model <- df$train_df %>% create_model(custom = TRUE, name = "my_custom_model")
custom_model
```

Stack layers on top of your custom model:
```{r}
custom_model <- custom_model %>%
  stack_layers(layer_dropout(rate = 0.1)) %>% 
  stack_layers(layer_dense(units = 64, activation = 'relu'))
custom_model

# this works too
custom_model %>%
  stack_layers(layer_dropout(rate = 0.1), layer_dense(units = 64, activation = 'relu'))
```

You can also do it directly with keras-package:
```{r}
# here custom_model is a list containing a model and other useful parameters
new_outputs <- custom_model$model$output %>% # custom_model$model to access a model and $output to access the outputs of that model
  keras::layer_dropout(rate = 0.1) %>%
  keras::layer_dense(units = custom_model$num_outputs, activation = 'linear')

custom_model <- keras::keras_model(inputs = custom_model$model$input, outputs = new_outputs, name = "new_custom_model")
custom_model
```

We can also opt for setting up and training our model manually, instead of using methods. Note that after defining a model with keras::keras_model() the model no longer is of class ppred_model.
```{r}
custom_model %>% class
compile(object=custom_model, optimizer = "adam", 
        loss = loss_sparse_categorical_crossentropy(), 
        metrics = metric_sparse_categorical_crossentropy())

# fit(object=custom_model$model)
# predict()
# evaluate()
```

To further use the provided methods for __fit()__, __predict()__ and __evaluate()__ or have access to the specifications of a event log (ex. num_outputs, max_case_length, etc. ) use __map_attributes()__.
```{r}
#map_attributes(custom_model, processed_df = df)
```

# Some other functions
<details>
<summary>
</summary>
<p>
processpredictR also provides a number of other exported functions, that are abstracted from the user, but may come in handy.

* __tokenize()__: returns tokenized sequences of activities, matrices of numeric and categorical features and a target variable. 
```{r}
tokens <- tokenize(df_next_time$train_df)
map(tokens, head)
```

```{r, echo=FALSE}
df <- prepare_examples(patients, "next_activity")
```


* __max_case_length()__: returns a number of activities in a longest trace.
```{r}
max_case_length(df)
```

* __num_outputs()__: returns a number of outputs based on the task.
```{r}
num_outputs(df)
```

* __create_vocabulary()__: returns a vocabulary, i.e a list of nested list representing inputs and outputs, where the index of an element can be seen as a key. 
```{r}
create_vocabulary(df)
```

* __vocab_size()__: returns a size of a vocabulary inputs.
```{r}
vocab_size(df)
```


</p>
</details>


# Attribution
This repository is based on the ProcessTransformer (Python) library by Buksh et al. In particular, the developed application of the three process monitoring tasks _next_activity_, _next time_ and _remaining time_ was extended with the tasks _outcome_ and _remaining trace_. 
Citation: Zaharah A. Bukhsh, Aaqib Saeed, & Remco M. Dijkman. (2021). “ProcessTransformer: Predictive Business Process Monitoring with Transformer Network”. arXiv preprint arXiv:2104.00721




